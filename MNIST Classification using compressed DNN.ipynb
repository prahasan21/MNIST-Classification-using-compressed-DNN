{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdo4A6wdMjiD"
   },
   "source": [
    "\n",
    "#### Network Compression using SVD for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBrT2MYfLOyP"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "0kPML4kELQDA",
    "outputId": "7759463c-aea7-46f3-eb9a-845321bff92f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Reading the MNIST image files\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvotxgG_MAMV"
   },
   "source": [
    "Incorporated the Xavier initialization as it gives above 98% accuracy, as a baseline model to go forward with building a compressed network:<br>\n",
    "-> Input dimentsions = 784 <br>\n",
    "-> 5 hidden layers with 1024 hidden units each <br>\n",
    "-> Xavier Initialization for and weights Random normal initialization for biases<br>\n",
    "-> Adam optimizer and cross entropy loss function with logits.<br>\n",
    "-> ReLu activation function for all the layers.<br>\n",
    "-> Default learning rate of Adam Optimizer = 0.001 <br>\n",
    "-> Output Layer dimensions = 10 (0-9 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_L7eKUyLQFY"
   },
   "outputs": [],
   "source": [
    "#Defining the neural network for baseline\n",
    "#MNIST Classification 0-9 - 10 classes one hot encoded\n",
    "\n",
    "#Hidden layers:\n",
    "n_nodes_hl1 = 1024\n",
    "n_nodes_hl2 = 1024\n",
    "n_nodes_hl3 = 1024\n",
    "n_nodes_hl4 = 1024\n",
    "n_nodes_hl5 = 1024\n",
    "\n",
    "\n",
    "#Classes\n",
    "n_classes = 10\n",
    "batch_size = 150\n",
    "n_epochs = 30\n",
    "\n",
    "x = tf.placeholder('float',[None,784])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "20_YAXQZLQIQ",
    "outputId": "2ca74575-558c-4c3f-cf55-ed6af5859a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 185.48808801174164\n",
      "epoch 1 loss 43.385691637173295\n",
      "epoch 2 loss 28.73910706723109\n",
      "epoch 3 loss 21.38529244204983\n",
      "epoch 4 loss 18.401518031721935\n",
      "epoch 5 loss 13.538994213799015\n",
      "epoch 6 loss 12.057347120397026\n",
      "epoch 7 loss 10.034107898332877\n",
      "epoch 8 loss 9.726478597061941\n",
      "epoch 9 loss 8.027179803873878\n",
      "epoch 10 loss 8.71093807775469\n",
      "epoch 11 loss 8.18182961284765\n",
      "epoch 12 loss 6.597328985400964\n",
      "epoch 13 loss 7.298225230748358\n",
      "epoch 14 loss 7.506153215043014\n",
      "epoch 15 loss 5.928964845541486\n",
      "epoch 16 loss 6.438356898124766\n",
      "epoch 17 loss 5.234458309336333\n",
      "epoch 18 loss 6.148189006822577\n",
      "epoch 19 loss 4.687322076832061\n",
      "epoch 20 loss 4.5483121237957675\n",
      "epoch 21 loss 4.201057384332671\n",
      "epoch 22 loss 6.912414456704937\n",
      "epoch 23 loss 3.7581618744316074\n",
      "epoch 24 loss 4.428032477248053\n",
      "epoch 25 loss 4.013434745966151\n",
      "epoch 26 loss 4.411360191781569\n",
      "epoch 27 loss 4.3473117967575945\n",
      "epoch 28 loss 8.036905290613504\n",
      "epoch 29 loss 2.773205591518007\n",
      "Accuracy 98.07999730110168 %\n"
     ]
    }
   ],
   "source": [
    "random.seed(50)\n",
    "data = x\n",
    "h1 = {'w':tf.Variable(tf.contrib.layers.xavier_initializer()([784,n_nodes_hl1])),'b':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "h2 = {'w':tf.Variable(tf.contrib.layers.xavier_initializer()([n_nodes_hl1,n_nodes_hl2])),'b':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "h3 = {'w':tf.Variable(tf.contrib.layers.xavier_initializer()([n_nodes_hl2,n_nodes_hl3])),'b':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "h4 = {'w':tf.Variable(tf.contrib.layers.xavier_initializer()([n_nodes_hl3,n_nodes_hl4])),'b':tf.Variable(tf.random_normal([n_nodes_hl4]))}\n",
    "h5 = {'w':tf.Variable(tf.contrib.layers.xavier_initializer()([n_nodes_hl4,n_nodes_hl5])),'b':tf.Variable(tf.random_normal([n_nodes_hl5]))}\n",
    "output_layer = {'w':tf.Variable(tf.contrib.layers.xavier_initializer()([n_nodes_hl5,n_classes])),'b':tf.Variable(tf.random_normal([n_classes]))}\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#(input_data*weights) + biases\n",
    "l1 = tf.add(tf.matmul(data, h1['w']),h1['b'])\n",
    "l1 = tf.nn.relu(l1)\n",
    "l2 = tf.add(tf.matmul(l1, h2['w']),h2['b'])\n",
    "l2 = tf.nn.relu(l2)\n",
    "l3 = tf.add(tf.matmul(l2, h3['w']),h3['b'])\n",
    "l3 = tf.nn.relu(l3)\n",
    "l4 = tf.add(tf.matmul(l3, h4['w']),h4['b'])\n",
    "l4 = tf.nn.relu(l4)\n",
    "l5 = tf.add(tf.matmul(l4, h5['w']),h5['b'])\n",
    "l5 = tf.nn.relu(l5)\n",
    "\n",
    "output = tf.add(tf.matmul(l5, output_layer['w']),output_layer['b'])\n",
    "\n",
    "pred = output\n",
    "cost_func = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) )\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost_func)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  epoch_loss = 0\n",
    "  for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "    epoch_x,epoch_y = mnist.train.next_batch(batch_size)\n",
    "    _,c = sess.run([optimizer,cost_func], feed_dict = {x:epoch_x, y:epoch_y})\n",
    "    epoch_loss += c\n",
    "  print('epoch',epoch,'loss',epoch_loss)\n",
    "correct = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "print('Accuracy',sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A86D0orsMXBe"
   },
   "source": [
    "Ran Singular Value Decomposition on all the five layers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HkLV6ee7LQKi"
   },
   "outputs": [],
   "source": [
    "s1, u1, v1 = tf.svd(h1['w'])\n",
    "\n",
    "s2, u2, v2 = tf.svd(h2['w'])\n",
    "\n",
    "s3, u3, v3 = tf.svd(h3['w'])\n",
    "\n",
    "s4, u4, v4 = tf.svd(h4['w'])\n",
    "\n",
    "s5, u5, v5 = tf.svd(h5['w'])\n",
    "\n",
    "d = tf.placeholder(tf.int32, shape = (), name = \"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOGnbKjhLQNT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "l1_svd = tf.matmul(tf.multiply(tf.matmul(x, u1[:,:d]), s1[:d]), tf.transpose(v1)[:d,:]) + h1['b']\n",
    "l1_svd = tf.nn.relu(l1_svd)\n",
    "l2_svd = tf.matmul(tf.multiply(tf.matmul(l1_svd, u2[:,:d]), s2[:d]), tf.transpose(v2)[:d,:]) + h2['b']\n",
    "l2_svd = tf.nn.relu(l2_svd)\n",
    "l3_svd = tf.matmul(tf.multiply(tf.matmul(l2_svd, u3[:,:d]), s3[:d]), tf.transpose(v3)[:d,:]) + h3['b']\n",
    "l3_svd = tf.nn.relu(l3_svd)\n",
    "l4_svd = tf.matmul(tf.multiply(tf.matmul(l3_svd, u4[:,:d]), s4[:d]), tf.transpose(v4)[:d,:]) + h4['b']\n",
    "l4_svd = tf.nn.relu(l4_svd)\n",
    "l5_svd = tf.matmul(tf.multiply(tf.matmul(l4_svd, u5[:,:d]), s5[:d]), tf.transpose(v5)[:d,:]) + h5['b']\n",
    "l5_svd = tf.nn.relu(l5_svd)\n",
    "\n",
    "output_svd = tf.add(tf.matmul(l5_svd, output_layer['w']),output_layer['b'])\n",
    "pred_svd = output_svd\n",
    "\n",
    "cost_func_svd = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=pred_svd, labels=y) )\n",
    "optimizer_svd = tf.train.AdamOptimizer().minimize(cost_func_svd)\n",
    "\n",
    "correct_svd = tf.equal(tf.argmax(pred_svd, 1), tf.argmax(y, 1))\n",
    "accuracy_svd = tf.reduce_mean(tf.cast(correct_svd, 'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGmueZleNIR8"
   },
   "source": [
    "1.5) As can be seen in the graph the best performance is for $D{full}$, and the worst performance is for D = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "vh_b3g7HLQPn",
    "outputId": "41cf45f5-00e3-4d7f-a9de-e9aa602c70e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Test Accuracy')"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH/mpnzBae4ZfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# D_full being 1024\n",
    "D_vals = [10, 20, 50, 100, 200, 1024]\n",
    "accu = []\n",
    "\n",
    "for i in range(len(D_vals)):\n",
    "  accu.append(sess.run(accuracy_svd, feed_dict = {d: D_vals[i], x:mnist.test.images, y:mnist.test.labels})) \n",
    "plt.plot(D_vals, accu, 'rx-')\n",
    "plt.xlabel(\"D\")\n",
    "plt.ylabel(\"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCJor5SBLt8b"
   },
   "source": [
    "#### 1.6) Fixing the value of D = 20\n",
    "\n",
    "Following the first approach, as stated in 1.6(a), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "daIvix_ELQSW"
   },
   "outputs": [],
   "source": [
    "\n",
    "S1, U1, V1, b1 = sess.run([s1, u1, v1, h1['b']])\n",
    "S2, U2, V2, b2 = sess.run([s2, u2, v2, h2['b']])\n",
    "S3, U3, V3, b3 = sess.run([s3, u3, v3, h3['b']])\n",
    "S4, U4, V4, b4 = sess.run([s4, u4, v4, h4['b']])\n",
    "S5, U5, V5, b5 = sess.run([s5, u5, v5, h5['b']])\n",
    "opW, opB = output_layer['w'],output_layer['b']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5RWuDhQLaED"
   },
   "source": [
    "As the network initially, was slow in training, I have casted the S,U,V under tf.Variable() and then re-structured it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "rbWTGtBLLQU9",
    "outputId": "063acb59-4224-4e68-a8e7-f00198dbec21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py:2825: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    }
   ],
   "source": [
    "d = 20\n",
    "\n",
    "V1 = tf.Variable((S1[:d]*V1[:, :d]).T)\n",
    "U1 = tf.Variable(U1[:,:d])\n",
    "b1 = tf.Variable(b1)\n",
    "\n",
    "V2 = tf.Variable((S2[:d]*V2[:, :d]).T)\n",
    "U2 = tf.Variable(U2[:,:d])\n",
    "b2 = tf.Variable(b2)\n",
    "\n",
    "V3 = tf.Variable((S3[:d]*V3[:, :d]).T)\n",
    "U3 = tf.Variable(U3[:,:d])\n",
    "b3 = tf.Variable(b3)\n",
    "\n",
    "V4 = tf.Variable((S4[:d]*V4[:, :d]).T)\n",
    "U4 = tf.Variable(U4[:,:d])\n",
    "b4 = tf.Variable(b4)\n",
    "\n",
    "V5 = tf.Variable((S5[:d]*V5[:, :d]).T)\n",
    "U5 = tf.Variable(U5[:,:d])\n",
    "b5 = tf.Variable(b5)\n",
    "\n",
    "opW = tf.Variable(opW)\n",
    "opB = tf.Variable(opB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTo8_iCQLQXx"
   },
   "outputs": [],
   "source": [
    "to_be_init_vars = [V1, U1, V2, U2, V3, U3, V4, U4, V5, U5, b1, b2, b3, b4, b5, opW, opB]\n",
    "\n",
    "init_1 = tf.variables_initializer(to_be_init_vars)\n",
    "sess.run(init_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dusaMdjmLQaQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "l1_svd_20 = tf.add(tf.matmul(x,(tf.matmul(U1,V1))), b1)\n",
    "l1_svd_20 = tf.nn.relu(l1_svd_20)\n",
    "\n",
    "l2_svd_20 = tf.add(tf.matmul(l1_svd_20,(tf.matmul(U2,V2))), b2)\n",
    "l2_svd_20 = tf.nn.relu(l2_svd_20)\n",
    "\n",
    "l3_svd_20 = tf.add(tf.matmul(l2_svd_20,(tf.matmul(U3,V3))), b3)\n",
    "l3_svd_20 = tf.nn.relu(l3_svd_20)\n",
    "\n",
    "l4_svd_20 = tf.add(tf.matmul(l3_svd_20,(tf.matmul(U4,V4))), b4)\n",
    "l4_svd_20 = tf.nn.relu(l4_svd_20)\n",
    "\n",
    "l5_svd_20 = tf.add(tf.matmul(l4_svd_20,(tf.matmul(U5,V5))), b5)\n",
    "l5_svd_20 = tf.nn.relu(l5_svd_20)\n",
    "\n",
    "l_op_20 = tf.matmul(l5_svd_20, opW)+ opB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0egW7itLQc7"
   },
   "outputs": [],
   "source": [
    "#Setting a low learning rate of 0.00001\n",
    "\n",
    "init_vars = set(tf.global_variables())\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = l_op_20, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(0.00001).minimize(cost)\n",
    "sess.run(tf.variables_initializer(set(tf.global_variables()) - init_vars))\n",
    "\n",
    "correct = tf.equal(tf.argmax(l_op_20, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "colab_type": "code",
    "id": "1XNU4UBrLQfW",
    "outputId": "1fd6ff96-ee4d-4009-a270-497c56a51016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 epoch loss 89.44561988860369\n",
      "epoch 1 epoch loss 52.2577834315598\n",
      "epoch 2 epoch loss 44.71851280145347\n",
      "epoch 3 epoch loss 38.5176507383585\n",
      "epoch 4 epoch loss 35.71346833743155\n",
      "epoch 5 epoch loss 32.921982002444565\n",
      "epoch 6 epoch loss 30.683997925836593\n",
      "epoch 7 epoch loss 28.378394033759832\n",
      "epoch 8 epoch loss 27.095213156193495\n",
      "epoch 9 epoch loss 25.532018517144024\n",
      "epoch 10 epoch loss 24.60654608020559\n",
      "epoch 11 epoch loss 23.021764043718576\n",
      "epoch 12 epoch loss 22.58966112881899\n",
      "epoch 13 epoch loss 21.595755398273468\n",
      "epoch 14 epoch loss 20.493986262008548\n",
      "epoch 15 epoch loss 20.02113056834787\n",
      "epoch 16 epoch loss 19.33806428220123\n",
      "epoch 17 epoch loss 18.682646145112813\n",
      "epoch 18 epoch loss 17.852592464303598\n",
      "epoch 19 epoch loss 17.438902121735737\n",
      "epoch 20 epoch loss 16.517872279859148\n",
      "epoch 21 epoch loss 16.701299568405375\n",
      "epoch 22 epoch loss 15.540785312652588\n",
      "epoch 23 epoch loss 15.481235947925597\n",
      "epoch 24 epoch loss 14.717421644134447\n",
      "epoch 25 epoch loss 14.742009248351678\n",
      "epoch 26 epoch loss 13.78997271717526\n",
      "epoch 27 epoch loss 13.768156934878789\n",
      "epoch 28 epoch loss 13.625718301627785\n",
      "epoch 29 epoch loss 12.821626653894782\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "        epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "        epoch_loss += c\n",
    "    print(\"epoch\",epoch, \"epoch loss\", epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RUdFH-boLQh1",
    "outputId": "54d5aa43-6e30-479f-db39-5c0e65ddfb5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for D = 20 is  97.79000282287598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "print('Test Accuracy for D = 20 is ', acc*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zx3vnzFNNUhL"
   },
   "source": [
    "The compressed neural network gives an accuracy which is 97.7%accuracycompared to the fully connected baseline network (98% accuracy). But the compressed network uses only 4% of the memory that of fully connected network, which is appreciable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeZQHgD4LROG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep_Learning_HW3_MNIST_SDN_Prahasan_Gadugu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
